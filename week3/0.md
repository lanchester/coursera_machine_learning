# 分類
## データセット
分類のデータセットのラベルは0か1のどちらかになります。そのため0未満1超過になることがある回帰分析のモデルは適しません。

2つのクラスの分類はバイナリ分類、それ以上をマルチクラス分類という。

## 仮説
分類の仮説関数は $0 \leqq y \leqq 1$ を満たすシグモイド関数 $g(z)=1/(1+e^{-z})$ (別名ロジスティック関数)が使われます。

$h(x)=\theta^Tx$

$h(x)=g(\theta^Tx)$

つまり仮説は $h(x)=1/(1+e^{-\theta^Tx})$ になります。

分類の仮説 $h(x)$ の値は予測が1になる確率として扱います。

$h(x)=P(y=1|x; \theta)$ (xがある値のときyが1になる確率)

0に分類される確率は上記を1から引いた値になります。

$\theta^Tx \leqq 0$のとき $y=1$ と予測する

## 決定境界

上記の $g(\theta^Tx)$ の中身の $\theta^Tx$ を分類モデルでは決定境界(decision boundary)と言います。

決定境界はデータセットではなくパラメータの性質です。パラメータはデータセットによって決定されますが、一度決定されたパラメータによる決定境界はたとえデータセットを取り除いても影響はありません。

### 非線形決定境界

回帰分析と同様に分類でも高次の関数を決定境界に使える

分類　コスト関数

分類のコスト関数を $(h(x) - y)^2$ とすると $(シグモイド関数 - y)^2$ の和になり、これは局所最適がいくつもある複雑な非凸関数である。これに最急降下法を適用してもグローバルな最小値が得られる保証はない。

そこで以下のようにコスト関数を定義する
$$
-log(h_\theta(x))　　if　y=1
$$
$$
-log(1 - h_\theta(x)) 　　if　y=0
$$

これは最急降下法が適用できる凸関数になり、それぞれ予測が間違っていた場合は確率が高いほど高いコスト(無限大)がかかる

コスト関数と最急降下法の簡略化

コスト関数を一行で表すのは省略

なぜそのコスト関数が良いのか？→最尤推定法により導かれたもの。最尤推定法は様々なモデルのパラメータΘを効率的に探す方法で、これで探しだした関数は凸関数になる

分類のコスト関数の最小化にも最急降下法が使え、回帰分析の時の式とほぼ同じになる

違う点は仮説関数が $\theta^Tx$かシグモイド関数かの違いだけである

最急降下法を使う際に各パラメータの $\theta_0,\theta_1, ...$ を更新するにはforループよりベクトル化して一度に更新するほうが良い

## 分類の最適化

最急降下法ではコスト関数の偏微分の項のみを計算すればよく、$\theta$ を計算する必要はない

ただし収束モニタリングをする場合は必要

最急降下法以外にもより洗練されたアルゴリズムとして共役勾配法、BFGS、L-BFGSなどがある

これらはどれも
- $\alpha$ を選ぶ必要はない
- 最急降下法よりも早く収束する
- 非常に複雑

詳細を理解できなくても実装することは可能

実際の学習時にはこれらのアルゴリズムを使うことが多い

# マルチクラス分類

メールの自動タグ付けや晴れ、雨、曇り、雪等の天気の分類など複数のクラスの分類

one-vs-all、つまりある一つとそれ以外のような決定境界を定義する。3つの分類なら3つのバイナリ分類を行う

3つの分類器を学習させ、予測時にはその分類器の中で最も確率の高いものを出す

## オーバーフィッティング

ある予測の仮説について関数の次数、パラメータ、フィーチャーが足りず予測として適切でない場合、アンダーフィッティング、または高バイアスであるという

逆に次数、パラメータ、フィーチャー等が多すぎて学習データに完全にフィットする場合、オーバーフィッティング、または高バリアントであるという

オーバーフィッティングの場合新しく予測のためのデータセットを与えても正しく予測ができない

フィーチャーが多い場合、データをプロットすることは難しくどのフィーチャーを使うべきかをグラフから判断することはできない

## オーバーフィッティングの回避法

1. フィーチャー数を減らす
- 実際に人がフィーチャーを見て必要そうなものを判断する
- モデル選択アルゴリズムを使う。自動的に必要なフィーチャーを選び出してくれる

2. 正規化(regularization)を行う
- フィーチャーを維持したままでパラメータ $\theta_j$ の倍率を下げる
- 多くのフィーチャーがあり、それぞれが予測に少しずつ貢献している場合に有効

## 正規化時のコスト関数

オーバーフィッティングを避けるため、通常のコスト関数に対して高い次数のパラメータ $\theta_j$ $1000\theta_j^2$ をプラスする。高い次数に対してペナルティを与える。こうすることで $\theta_j$ の影響を小さくする

パラメータを小さくすることは

- 仮説がシンプルになる
- オーバーフィッティングしづらくなる

大量のフィーチャー、パラメータのうちどれを縮めるべきか人が判断するのは難しい。そのため全てのパラメータを縮めて確かめる

## 正規化時のコスト関数
コスト関数に正規化項としてパラメータを縮めるため全てのパラメータの二乗の和を足す。和は $i=1$ から始め、$\theta_0$ は縮めない

正規化パラメータ $\lambda$ の2つの役割
- トレーニングセットに上手くフィットさせる
- パラメータを小さく保つ

$\lambda$ に非常に大きな値を設定すると、各Θに対して大きくペナルティを課すことになり値が0に近づくため、仮説はアンダーフィッティングする

そのため $\lambda$ の値はちょうど良いものを選ぶ必要がある

## 正規化された線形回帰

正規化した最急降下法では $\theta_0$ に対してはペナルティは課さない。そしてそれ以外の $\theta$ に対して $-\frac{\lambda}{m}\theta_j$ を足す

これは $J(\theta)$ の $\theta_j$ による偏微分と一致する

まとめると正規化された最急降下法はつぎのようになる。

$$
\theta_j := \theta_j(1-\alpha\frac{\lambda}{m}) - \alpha\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)}
$$

$\theta_j(1 - \alpha\frac{\lambda}{m})$ の項は $\theta_j$ をわずかに0方向に縮める働きをする

## 正規化時の正規方程式

正規方程式 $\theta = (X^TX)^{-1}X^Ty$ に正規化項を加えると以下のようになる。
$$
\theta = (X^TX + \lambda A)^{-1}X^Ty
$$
ただしAは
$$
A = \left[ \begin{array}{ccc} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{array} \right]
$$

 のようなn+1次元の単位行列

これはXが特異行列の場合にも適用できる

## 正規化時の分類

$$
\theta_0 := \theta_0 - \alpha \frac{1}{m} \sum_{i = 1}^{m}(h_\theta(x^{(i)}) - y^{(i)})x_0^{(i)}
$$
$$
\theta_j := \theta_j - \alpha \left[ \frac{1}{m} \sum_{i = 1}^{m}(h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)} + \frac{\lambda}{m}\theta_j \right]　(j = 1,2,...,n)
$$

$\frac{\lambda}{m}\theta_j$ は正規化項、 $h_\theta x = g(\theta^Tx)$、 $[　]$ 内の項は  $J(\theta)$ の $\theta_j$ についての偏微分

## 正規化時の最適化

分類のコスト関数を最小化するコードを実装する際には
1. コスト関数そのものの値
2. n = 1,2,...n までの $\frac{\delta}{\delta\theta_n}$ までの偏微分項の値
を返すコードを書く

この時のコスト関数には末尾に正規化項  $\frac{\lambda}{2m}\sum_{j=1}^{n}\theta_j^2$ を加える
