## バックプロパゲーションの直感

### フォワードプロパゲーション

![2016-07-26 16 17 28](https://cloud.githubusercontent.com/assets/6447085/17129162/7c5c0e5a-534c-11e6-9a75-7ebfadf215fa.png)

フォワードプロパゲーションでは前のレイヤーで計算されたアクティベート関数にウェイトを与えた値 $z$ が次のレイヤーに渡され、その $z$ を用いてアクティベート関数が計算される

バックプロパゲーションでもやることは同じようなもので計算の流れが逆になる

### バックプロパゲーションは何をしているか？

![2016-07-26 16 18 48](https://cloud.githubusercontent.com/assets/6447085/17129207/aaf04a24-534c-11e6-9f2e-6382814608a2.png)

単純なバイナリ分類の場合、i番目のトレーニングセット $^{(i)}, y^{(i)}$ に対応する場合のコストは以下のように表される。
$cost(i) = y^{(i)}logh_\Theta(z^{(i)}) + (1-y^{(i)})logh_\Theta(x^{(i)})$

この式の値は実際には二乗誤差とほぼ同じなのでそう考えても良い。

### フォワードプロパゲーション

![2016-07-26 16 20 02](https://cloud.githubusercontent.com/assets/6447085/17129230/d51664e6-534c-11e6-9cb1-b324cf797ecc.png)

バックプロパゲーションでは $\delta_j^{(l)}$ を計算している。 これは $l$ 番目のレイヤーの $j$ 番目のユニットのアクティベート関数 $a_j^{(l)}$ の誤差と考えることが出来る。

正確に言うと $\delta_j^{(l)} = \frac{\partial}{\partial z_j≠{(l)}}$ つまり $z_j^{(l)}$ による偏微分係数である。

この偏微分係数はニューラルネットワークの中間計算に影響を与え、さらに最終的な出力 $h(x)$ に影響を与える。結果的にウェイトをどれだけ与えればよいかの指標となる。

バックプロパゲーションは出力例やの誤差から計算を始めていく。

$\delta_1^{(4)}$、 $\delta_1^{(3)}, \delta_2^{(3)}$ 、$\delta_1^{(2)}, \delta_2^{(2)}$ と伝播していく

$\delta $ は一つ後ろのレイヤーの各 $\delta$ に一つ前のレイヤーのウェイトを掛けた値の和になる。
