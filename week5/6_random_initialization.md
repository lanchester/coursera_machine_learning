## ランダム初期化

### $\Theta$ の初期値

![2016-07-26 16 29 45](https://cloud.githubusercontent.com/assets/6447085/17129457/3229c60e-534e-11e6-8aa8-f8309a73d491.png)

$\Theta $ には初期値が必要。全てを0で代入するのは分類の時には上手くいったが、ニューラルネットワークの場合はそうではない。

### 0 による初期化

![2016-07-26 16 30 23](https://cloud.githubusercontent.com/assets/6447085/17129472/475e6e4e-534e-11e6-81ce-79c82daf2818.png)

全てを0で代入すると隠れレイヤのアクティベート関数が全て同じになってしまい、そうなると $\delta $ も同じになってしまう。

これはつまり最急降下法などのアルゴリズムを走らせても各イテレーションで常に同じ値を得ることになる。

### ランダム初期化: 対称性の破れ

![2016-07-26 16 30 57](https://cloud.githubusercontent.com/assets/6447085/17129482/5b31b1a6-534e-11e6-9890-7beb2e639fa5.png)

上手く学習するためには $[-\epsilon, \epsilon ]$ の範囲のランダムな値を初期値に代入する。この$\epsilon $ は収束チェックで用いた値とは全く関係がない
