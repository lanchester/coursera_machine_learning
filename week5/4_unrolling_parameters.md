## 実装メモ: パラメータの展開

### 高度な最適化

![2016-07-26 16 21 52](https://cloud.githubusercontent.com/assets/6447085/17129282/1b2e8ab2-534d-11e6-9573-e2f2aa5950d2.png)

パラメータを受け取り、コスト関数とその微分を返す関数を実装して、fminuncのような最適化アルゴリズムに渡すことができる。これらはtheta やinitialTheta の値をベクトルで受けとる。

ニューラルネットワークではパラメータはすでにベクトルではなく行列になっているため、これをベクトルに変換する必要がある。

### 例

![2016-07-26 16 22 35](https://cloud.githubusercontent.com/assets/6447085/17129302/3326ff96-534d-11e6-965c-633958ae5b9e.png)

各レイヤーのパラメータ行列を一つの長いベクトルにまとめる。逆にパラメータ行列に戻したい場合は長いパラメータベクトルのうち該当する部分だけを抜き出す。

例えば $\Theta ^{(1)} \in \mathbb{R}^{10×11}$ なら1から110番目までの値を抜き出し、10×11行列として整形する。

### Octave デモ

### 学習アルゴリズム

![2016-07-26 16 23 48](https://cloud.githubusercontent.com/assets/6447085/17129329/5d3d74ea-534d-11e6-9b3e-5cda0532ea68.png)

コスト関数は thetaVec というベクトルを受けとる。これには行列が展開されたベクトルが入っている。このベクトルから reShape関数を用いて行列に変換し、コスト関数やその微分を計算する

最終的に偏微分項gradientVecを計算できる
