## まとめ

### ニューラルネットワークの学習

![2016-07-26 16 32 28](https://cloud.githubusercontent.com/assets/6447085/17129523/91887596-534e-11e6-9b4d-730b5e49e474.png)

学習する際にはまず入力レイヤ、隠れレイヤ、出力レイヤの数をどうするかアーキテクチャを選ぶ。入力レイヤはフィーチャーの数によって決まるため最初に決めておく。出力レイヤはマルチクラス分類のクラス分の数。

隠れレイヤは1つが一般的。2つ以上にする場合もユニット数を同じにすることが多い。隠れレイヤは多ければ多いほど良い結果を得られることが多いが、計算量が多くなるため左の基本のアーキテクチャでも問題ない。十分良い結果を得られる。

隠れユニットの数はフィーチャーと同じくらいか2～4倍程度が良い。

### ニューラルネットワークのトレーニング

![2016-07-26 16 32 58](https://cloud.githubusercontent.com/assets/6447085/17129542/b031f8b4-534e-11e6-9b99-5a95ed2ed070.png)

トレーニングの際は1～4の手順で実装する。また、最初に実装するときはベクトルを一度に更新する方法でなく、forループによる実装をおすすめする。

![2016-07-26 16 33 57](https://cloud.githubusercontent.com/assets/6447085/17129552/c79b00cc-534e-11e6-8073-3dafa1a8f681.png)


5は収束チェック、6では収束チェックをオフにする。ところで、ニューラルネットワークのコスト関数は凸関数ではないため、どのようなアルゴリズムでも局所最小に陥るが、その値はグローバル最適と比べてもとても良い値である。

### 最急降下法の収束図

![2016-07-26 16 34 43](https://cloud.githubusercontent.com/assets/6447085/17129578/e3b5f096-534e-11e6-96d3-6b0833dfa300.png)

$h_\Theta(x^{(i)})$ が $y^{(i)}$ と十分に近い時コスト関数は収束している。逆に遠い時はあまりフィットしていない。Θはランダムな点から初める。
