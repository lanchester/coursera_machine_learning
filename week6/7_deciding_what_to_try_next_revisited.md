## 次の一手(再訪)

### 学習アルゴリズムのデバッグ

![2016-08-01 15 50 58](https://cloud.githubusercontent.com/assets/6447085/17285755/c36ec686-57ff-11e6-9a9f-43f1a9938ec9.png)

・データを増やす
→高バリアンス時に有効
・フィーチャーを減らす
→高バリアンス時に有効
・フィーチャーを増やす
→高バイアス時に有効
・次数を増やす
→高バイアス時に有効
・ $\lambda$ を減らす
→高バイアス時に有効
・ $\lambda$ を増やす
→高バリアンス時に有効

### ニューラルネットワークとオーバーフィッティング

![2016-08-01 15 51 39](https://cloud.githubusercontent.com/assets/6447085/17285767/dc81d69a-57ff-11e6-9972-966f54a8c1e3.png)

ニューラルネットワークをフィッティングする際は、隠れレイヤが1つでユニットが少ない小さいネットワークから試す方法がある。これはアンダーフィッティングしがちだが計算量が少なく済む。

もう一つは隠れレイヤやユニットが多いネットワークを選ぶ。これはオーバーフィッティングしやすく計算量も高くつきやすいが、実際にはそれほど問題にはならない。

ニューラルネットワークはネットワークが大きいほうが良く、オーバーフィッティングした場合は $\lambda$ で対処する。通常は大きめのネットワークに正規化項を加えるのが効果的。

隠れレイヤの数を選ぶ際は通常1つでも実用的だが、データセットを3つにわけ、隠れレイヤが1つの場合、2つ、3つの場合と、それぞれ場合を見てパフォーマンスが良いものを選択する。
