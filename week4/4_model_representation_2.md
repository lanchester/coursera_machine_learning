### ベクトル化された実装

<img width="962" alt="2016-07-19 20 49 58" src="https://cloud.githubusercontent.com/assets/6447085/16948906/653407a0-4df2-11e6-84b0-83910445cf77.png">

個々のユニットにおけるアクティベート関数 $a_i^{(j)} = g(z_i^{(j)})$ の $z_i^{(j)}$ は単に一つ前のレイヤーで重み付けされたフィーチャーの線形和である。

$$
z^{(j)} =(一つ前のレイヤーのウェイト)×(一つ前のレイヤーのアクティベート関数)
$$
$$
z^{(2)} = \theta^{(1)}x^{(1)}
$$
この $x^{(1)}$ はレイヤー1のアクティベート関数として考えることが出来るので次のように書き直せる。
$$
z^{(2)} = \theta^{(1)}a^{(1)}
$$
$$
a^{(2)} = g(z^{(2)})
$$

ここで $a^{(2)}$、 $z^{(2)}$ は共に3次元のベクトルなので、 $a^{(2)}_0=1$ のバイアスユニットを加えると4次元のフィーチャーベクトルとすることができる。これは出力レイヤの形に合わせるため。レイヤ3のアクティベート関数は
$$
z^{(3)} = \theta^{(2)}a^{(2)}
$$
となり、最終的に出力レイヤは
$$
h_\theta(x)=a^{(3)}=g(z^{(3)})
$$
という仮説による計算結果を出力する。

このプロセスをフォワードプロパゲーションという。

### フィーチャー自体の学習

<img width="963" alt="2016-07-19 21 12 19" src="https://cloud.githubusercontent.com/assets/6447085/16949432/bac2c640-4df5-11e6-9f0e-7f3c17fa15d8.png">

ニューラルネットワークは実際には分類とほぼ同じことを行う。違いはフィーチャーが $x$ か $a$ と言う点だけ。ただし、ニューラルネットワークではフィーチャーを制限する代わりに、独自のアクティベート関数によるフィーチャーaを与えることができるので、より良い仮説が得られる。ニューラルネットワークはどんなフィーチャーでも一旦学習するという柔軟性を持っている。

### 他のニューラルネットワークアーキテクチャ

<img width="958" alt="2016-07-19 21 19 05" src="https://cloud.githubusercontent.com/assets/6447085/16949573/781277c2-4df6-11e6-98dc-59ffd0432741.png">

ニューロン同士をどう結びつけるかはアーキテクチャによって異なる。隠れレイヤは複数あってもよく、ニューロンの数も各レイヤ毎に異なっていても良い。
